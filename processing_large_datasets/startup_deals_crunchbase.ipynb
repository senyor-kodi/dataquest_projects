{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1ea3ba-cfd6-4d39-8daa-91c7a518b600",
   "metadata": {},
   "source": [
    "# Analyzing Startup Fundraising Deals from Crunchbase\n",
    "\n",
    "Every year, thousands of startup companies raise financing from investors. Each time a startup raises money, we refer to the event as a fundraising round. Crunchbase is a website that crowdsources information on the fundraising rounds of many startups. The Crunchbase user community submits, edits, and maintains most of the information in Crunchbase.\n",
    "\n",
    "In return, Crunchbase makes the data available through a Web application and a fee-based API. Before Crunchbase switched to the paid API model, multiple groups crawled the site and released the data online. Because the information on the startups and their fundraising rounds is always changing, the data set we'll be using isn't completely up to date.\n",
    "\n",
    "The data set of investments we'll be exploring is current as of October 2013. You can download it from [GitHub](https://github.com/datahoarder/crunchbase-october-2013).\n",
    "\n",
    "Since the data set contains over 50,000 rows, we will need to read the data set into dataframes using 5,000 row chunks to ensure that each chunk consumes much less than 10 megabytes of memory. For each chunk, we will compute:\n",
    "\n",
    "- each chunk's missing value counts\n",
    "- each chunk's memory footprint\n",
    "- the total memory footprint of all of the chunks combined\n",
    "\n",
    "We will then analyze which column(s) we can drop because they aren't useful for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4d1406-aaa3-4988-b134-af23edb5a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_iter = pd.read_csv('crunchbase-investments.csv', chunksize=5000, encoding='ISO-8859-1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d5b2b0-f2e0-41ff-942d-2408f6cb5c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company_country_code          1\n",
       "company_name                  1\n",
       "company_permalink             1\n",
       "company_region                1\n",
       "investor_region               2\n",
       "investor_permalink            2\n",
       "investor_name                 2\n",
       "funded_quarter                3\n",
       "funded_at                     3\n",
       "funded_month                  3\n",
       "funded_year                   3\n",
       "funding_round_type            3\n",
       "company_state_code          492\n",
       "company_city                533\n",
       "company_category_code       643\n",
       "raised_amount_usd          3599\n",
       "investor_country_code     12001\n",
       "investor_city             12480\n",
       "investor_state_code       16809\n",
       "investor_category_code    50427\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing value counts calculation\n",
    "\n",
    "mv_list = []\n",
    "for chunk in chunk_iter:\n",
    "    mv_list.append(chunk.isnull().sum())\n",
    "    \n",
    "mv_counts = pd.concat(mv_list)\n",
    "unique_mv_counts = mv_counts.groupby(mv_counts.index).sum()\n",
    "unique_mv_counts.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cddb65-d374-4366-9ec9-13dc55930fd8",
   "metadata": {},
   "source": [
    "Any columns that have +90% of missing values are likely not going to be of much use for our analysis and therefore we will drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da5a27b6-b7fc-4556-9e01-1bd93facd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['investor_permalink', 'company_permalink', 'investor_category_code'] #columns to drop\n",
    "keep_cols = chunk.columns.drop(drop_cols).to_list() #columsn to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb0027a-fb3c-49b2-bdd9-49f322d7dd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company_permalink         4057788\n",
       "company_name              3591326\n",
       "company_category_code     3421104\n",
       "company_country_code      3172176\n",
       "company_state_code        3106051\n",
       "company_region            3411545\n",
       "company_city              3505886\n",
       "investor_permalink        4980548\n",
       "investor_name             3915666\n",
       "investor_category_code     622424\n",
       "investor_country_code     2647292\n",
       "investor_state_code       2476607\n",
       "investor_region           3396281\n",
       "investor_city             2885083\n",
       "funding_round_type        3410707\n",
       "funded_at                 3542185\n",
       "funded_month              3383584\n",
       "funded_quarter            3383584\n",
       "funded_year                422960\n",
       "raised_amount_usd          422960\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory footprint per column\n",
    "chunk_iter = pd.read_csv('crunchbase-investments.csv', chunksize=5000, encoding='ISO-8859-1')\n",
    "\n",
    "counter = 0\n",
    "series_memory_fp = pd.Series(dtype='float64')\n",
    "for chunk in chunk_iter:\n",
    "    if counter == 0:\n",
    "        series_memory_fp = chunk.memory_usage(deep=True)\n",
    "    else:\n",
    "        series_memory_fp += chunk.memory_usage(deep=True)\n",
    "    counter += 1\n",
    "\n",
    "series_memory_fp = series_memory_fp.drop('Index')\n",
    "series_memory_fp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488c0f57-e361-494d-843a-75eb41552165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory consumed is: 56.98753070831299\n"
     ]
    }
   ],
   "source": [
    "print(f'Total memory consumed is: {series_memory_fp.sum() / (1024 * 1024)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e1cedfa-8dfa-4822-b95b-bd4835bbeaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(keep_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1869f9-e035-4a84-b2b6-8afb86186e52",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "Let's now get familiar with the column types before adding the data into SQLite. We will:\n",
    "\n",
    "- Identify the types for each column.\n",
    "- Identify the numeric columns we can represent using more space efficient types.\n",
    "- For text columns:\n",
    "    - Analyze the unique value counts across all of the chunks to see if we can convert them to a numeric type.\n",
    "    - See if we clean clean any text columns and separate them into multiple numeric columns without adding any overhead when querying.\n",
    "\n",
    "The overall memory the data consumed should stay under 10 megabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c7a8e6d-06a8-4eaf-93b1-7f8840674d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company_name': {'object'},\n",
       " 'company_category_code': {'object'},\n",
       " 'company_country_code': {'object'},\n",
       " 'company_state_code': {'object'},\n",
       " 'company_region': {'object'},\n",
       " 'company_city': {'object'},\n",
       " 'investor_name': {'object'},\n",
       " 'investor_country_code': {'float64', 'object'},\n",
       " 'investor_state_code': {'float64', 'object'},\n",
       " 'investor_region': {'object'},\n",
       " 'investor_city': {'float64', 'object'},\n",
       " 'funding_round_type': {'object'},\n",
       " 'funded_at': {'object'},\n",
       " 'funded_month': {'object'},\n",
       " 'funded_quarter': {'object'},\n",
       " 'funded_year': {'float64', 'int64'},\n",
       " 'raised_amount_usd': {'float64'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv('crunchbase-investments.csv', chunksize=5000, encoding='ISO-8859-1', usecols=keep_cols)\n",
    "\n",
    "col_types = {}\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    for col in chunk.columns:\n",
    "        if col not in col_types:\n",
    "            col_types[col] = [str(chunk.dtypes[col])]\n",
    "        else:\n",
    "            col_types[col].append(str(chunk.dtypes[col]))\n",
    "\n",
    "uniq_col_types = {}\n",
    "for k,v in col_types.items():\n",
    "    uniq_col_types[k] = set(col_types[k])\n",
    "uniq_col_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da6430-457a-483e-ac69-f361e66330dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
